\documentclass{article}

\usepackage{amsmath,amssymb,amsthm,graphicx}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\cdf}{cdf}
\DeclareMathOperator{\disc}{disc}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\tr}{tr}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bx}{{\boldsymbol x}}
\newcommand{\by}{{\boldsymbol y}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\even}{\mathrm{even}}
\newcommand{\fr}{\mathrm{fr}}
\newcommand{\Leb}{\mathrm{Leb}}
\newcommand{\odd}{\mathrm{odd}}
\newcommand{\ST}{\mathrm{ST}}

\newtheorem{corollary}{Corollary}
\newtheorem{conjecture}{Conjecture}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}

\title{A counterexample relating exponential sums and discrepancy}
\author{Daniel Miller}

\begin{document}
\maketitle





For a prime $p$, let 
\begin{align*}
	T_p &= \left\{ \frac{a}{2\sqrt p} : a\in \bZ, |a|\leqslant 2\sqrt p\right\} \\
	\Theta_p &= \cos^{-1}\left(T_p \right) .
\end{align*}
Since applying continuous increasing functions preserves discrepancy, we have:
\begin{align*}
	\disc(T_p,\Leb) &\ll p^{-1/2} \\
	\disc\left(\Theta_p, \frac 1 2 \sin(t)\, \dd t\right) &\ll p^{-1/2} .
\end{align*}
We claim that starting with $\theta_2\in \Theta_2$, we can choose 
$\theta_p$ such that we preserve the inequalities:
\begin{align*}
	\frac{1}{4\log x} \leqslant \disc(\{\theta_p\}_{p\leqslant x}) \leqslant \frac{4}{\log x} \\
	\left| \sum_{p\leqslant x} U_1(\theta_p)\right| \leqslant 2 \sqrt{x}
\end{align*}
Recall that 
\[
	U_1(\theta) = \frac{\sin(2\theta)}{\sin\theta} .
\]
We can run this for all $p\leqslant {10}^{5}$. Recall that 
$\pi(10^5) \approx 10000$. 

Here is what we get:
\begin{figure}[ht]
\caption{Plot of $\sum_{p\leqslant x} U_1(\theta_p)$}
\centering
\includegraphics[width=0.8\textwidth]{sums_abs}
\end{figure}

\begin{figure}[ht]
\caption{Plot of $\disc(\{\theta_p\}_{p\leqslant x})$}
\centering
\includegraphics[width=0.8\textwidth]{ks_data}
\end{figure}

\begin{conjecture}
There exists a sequence of $\theta_p\in \Theta_p$ such that the following 
identities always hold:
\begin{align*}
	\frac{1}{4\log x} \leqslant \disc(\{\theta_p\}_{p\leqslant x}) \leqslant \frac{4}{\log x} \\
	\left| \sum_{p\leqslant x} U_1(\theta_p)\right| \leqslant 2 \sqrt{x} .
\end{align*}
\end{conjecture}

Next, choose $\bar\rho_l \colon G_\bQ \twoheadrightarrow \GL_2(\bF_l)$ to 
which we can apply Ramakrishna et.~al.'s machinery. Define 
\[
	\Theta_p(\bar\rho_l) = \left\{\cos^{-1}\left(\frac{a}{2\sqrt p}\right) : a\in \bZ, |a|\leqslant 2\sqrt p, a \equiv \tr \bar\rho_l(\fr_p)\pmod l\right\} .
\]
\begin{conjecture}
There exists a sequence of $\theta_p\in \Theta_p(\bar\rho_l)$ such that 
\begin{align*}
	\disc(\{\theta_p\}_{p\leqslant x}) =\Omega\left( \frac{1}{\log x} \right)\\
	\left| \sum_{p\leqslant x} U_1(\theta_p)\right| \ll \sqrt{x} .
\end{align*}
\end{conjecture}

\begin{corollary}
There exists an (infinitely ramified) Galois representation 
$\rho_l\colon G_\bQ \to \GL_2(\bZ_l)$ such that if we set 
$a_p = \tr\rho_l(\fr_p)$, then 
\begin{enumerate}
\item
$a_p\in \bZ$
\item
$|a_p| \leqslant 2\sqrt p$ .
\item
The $\theta_p = \cos^{-1}\left(\frac{a_p}{2\sqrt p}\right)$ satisfy 
\begin{align*}
	\disc(\{\theta_p\}_{p\leqslant x}) =\Omega\left( \frac{1}{\log x} \right)\\
	\left| \sum_{p\leqslant x} U_1(\theta_p)\right| \ll \sqrt{x} .
\end{align*}
and hence $L(\rho_l,s)$ satisfies the Riemann Hypothesis. 
\end{enumerate}
\end{corollary}





\section{Towards a proof}

Let $\bar\rho_l\colon G_\bQ \to \GL_2(\bF_l)$ be a Galois representation. For 
each prime $p$, define 
\[
	\Theta_p(l) = \left\{\cos^{-1}\left(\frac{a}{2\sqrt p}\right) : a\in \bZ, |a|\leqslant 2\sqrt p, a \equiv \tr \bar\rho_l(\fr_p)\pmod l\right\} .
\]
It is easy to check that 
\[
	\disc\left(\Theta_p(l),\frac 1 2 \sin(t)\, \dd t\right) \ll l p^{-1/2} .
\]
We are looking for a way to choose $\theta_p\in \Theta_p(l)$ such that 
\begin{enumerate}
\item
$\disc(\{\theta_p\}_{p\leqslant x})$ decays like $1/\log x$

\item
$\left| \sum_{p\leqslant x} U_1(\theta_p)\right|$ grows like 
$\sqrt x$. 
\end{enumerate}
To do this, suppose we have chosen $\{\theta_q\}_{q < p}$. In choosing 
$\theta_p$, we want to simultaneously move the discrepancy towards 
$1/\log p$, while making sure that the $U_1$-sum doesn't get too big. 

(Fact: if $\{x_1,\dots,x_N\}$ and $\{y_1,\dots,y_N\}$ are two sequences, 
then 
\[
	|\disc(\{x_1,\dots,x_N\}) - \disc(\{y_1,\dots,y_N\})| \leqslant 2\|x-y\|_\infty .
\]
)

It's actually quite simple. Note that:
\[
	U_1(\theta) = \frac{\sin(2\theta)}{\sin\theta} = -U_1(\pi-\theta) .
\]
The basic idea is: set $\theta_3 \approx \pi-\theta_2$, 
$\theta_7 \approx \pi-\theta_5$, etc.~and we can choose $\theta_2$, 
$\theta_5$ etc.~arbitrarily, meaning good discrepancy, while the sum should 
approximately cancel out. First, since $U_1$ has bounded derivative, we know 
that 
\[
	|U_1(\theta) - U_1(\varphi)| \ll |\theta-\varphi|
\]
So, if $p_1<p_2$ are sequential primes, we have 
\[
	|\theta_{p_2} - (\pi-\theta_{p_1})| \ll p_1^{-1/2} ,
\]
so 
\begin{align*}
	|U_1(\theta_{p_1}) + U_1(\theta_{p_2})| 
		&\leqslant |U_1(\theta_{p_1}) - U_1(\pi - \theta_{p_1})| + |U_1(\pi-\theta_{p_1}) - U_1(\theta_{p_2})| \\
		&\ll |\theta_{p_2} - (\pi-\theta_{p_1})| \\
		&\ll p_1^{-1/2} .
\end{align*}
So, 
\[
	\left| \sum_{p\leqslant x} U_1(\theta_p) \right| \ll \sum_{p\leqslant x} p^{-1/2} \ll \int_1^x t^{-1/2} \, \dd t \ll \sqrt x .
\]
(Same argument works for all $U_\odd$ because they all satisfy 
$U_\odd(\pi-\theta) = -U_\odd(\theta)$. In contrast, 
$U_\even(\pi-\theta) = U_\even(\theta)$.)





\section{A legit proof!}

\begin{theorem}
Fix a prime $l$. Suppose we have chosen, for all primes $p$, some arbitrary 
residue class $\bar a_p\in \bF_l$, and set 
\[
	\Theta_p(l) = \left\{\cos^{-1}\left(\frac{a}{2\sqrt p}\right) : a\in \bZ, |a|\leqslant 2\sqrt p, a \equiv \bar a_p\pmod l\right\} .
\]
Then there exists a choice of $\theta_p\in \Theta_p(l)$ such that 
\begin{enumerate}
\item
The sequence $\{\theta_p\}$ is equidistributed with respect to the Sato--Tate 
measure $\frac{2}{\pi}\sin^2\theta\, \dd \theta$. 

\item
The discrepancy $\disc(\{\theta_p\}_{p\leqslant x}, \ST) \gg \frac{1}{\log x}$. 

\item
$\left| \sum_{p\leqslant x} U_\mathrm{odd}(\theta_p)\right| \ll \sqrt x$. 
\end{enumerate}
\end{theorem}
\begin{proof}
Enumerate the primes $p_1 < p_2 < \cdots$. We will choose 
$\theta_{p_\odd}\in [0,\pi/2)$ so that the discrepancy of the sequence 
$\{\theta_{p_\odd}\}$ behaves as required in that interval. We'll then set 
$\theta_{p_{2i}}\approx \pi - \theta_{p_{2i-1}}$. 

Everything comes down to: if $p < q$ are sequential primes and we 
have already chosen $\theta_p$, we need to be able to choose $\theta_q$ so that 
$|U_1(\theta_p)+U_1(\theta_q)| \ll p^{-1/2}$. Since 
$\frac{\dd U_1}{\dd \theta} = -2\sin(\theta)$, we have (roughly)
\[
	|U_1(\theta)-U_1(\varphi)| \ll \max(\theta,\varphi) \cdot |\theta-\varphi| 
\]
for $\theta,\varphi\in [0,\pi/2)$. 

Start with $t_p = \frac{a_p}{2\sqrt p}$ and $t_q = \frac{a_q}{2\sqrt q}$. We 
can guarantee that $|t_p - (\pi-t_q)| \ll p^{-1/2}$. 

Fact: 
\[
	| \cos^{-1}(1-x) - \cos^{-1}(1-(x+\sqrt x))| \ll x^{1/5} .
\]
So roughly, 
\[
	|\theta_p - \theta_q| \ll p^{-1/5} ,
\]

After taking $\cos^{-1}$, 
all we can guarantee is that 
\[
	|\theta_p - \theta_q| \ll 
\]


--- --- --- --- --- --- --- --- --- --- --- --- 

Let's think systematically. We're picking $t_1$ and $t_2$ close to 
$1$, which is where $(\cos^{-1})'$ blows up. But there shouldn't be very many 
of them close to $1$. Aka, 
\begin{align*}
	\left|\frac{\#\{p \leqslant x : \theta_p \in [0,t)\}}{\pi(x)} - \int_0^t \dd\ST\right| 
		&\ll \frac{1}{\log x} \\ 
	\frac{\#\{p \leqslant x : \theta_p \in [0,t)\}}{\pi(x)} 
		&\ll t^2 + 	\frac{1}{\log x} .
\end{align*}

We want to know, given $x$, how small the smallest $\theta_p, p\leqslant x$ is. 
Roughly, for what $t$ is 
\[
	\# \{p \leqslant x: \theta_p\in [0,t)\} < 1? 
\]
We already know that 
\[
	\# \{p \leqslant x : \theta_p\in [0,t)\} \ll \frac{x}{\log x} \left( t^2 + \frac{1}{\log x}\right) .
\]
This is frustrating, because it means, essentially, that our convergence to 
the Sato--Tate measure is so slow (by design) that we can't \emph{ever} 
guarantee that no $\theta_p$ lies in some small interval. But there's 
something easier. For each $p\leqslant x$, we start by choosing 
$a_p\in \bZ$. How close can $a_p$ be to $2\sqrt p$? Numerical experiments 
\textbf{(prove this!)} show that for $t_p = \frac{a_p}{2\sqrt p}$, we have 
\[
	|1-t_p| \gg p^{-1/2} .
\]
This is key! That means $\theta_p$ won't be too small. In particular, we 
can control how close $\theta_p$ and $\theta_q$ will be. 

We already have chosen $\theta_p$. We want to choose $a_q$ so that 
$\cos^{-1}(\frac{a_q}{2\sqrt q}) \approx \pi-\theta_p$, i.e.
\[
	\frac{a_q}{2\sqrt q} \approx \sin(\theta_p) .
\]
We can ensure 
\[
	\left| \frac{a_q}{2\sqrt q} - \cos(\pi - \theta_p)\right| \ll p^{-1/2} .
\]
Moreover, we know that $|\pm 1 - \frac{a_q}{2\sqrt q}| \gg q^{-1/2}$, and 
likewise for $a_p$. Thus, 
\[
	|\theta_p - \theta_q| = \left|\cos^{-1}\left(\frac{a_p}{2\sqrt p}\right) - \pi +  \cos^{-1}\left(\frac{a_q}{2\sqrt q}\right)\right| \ll p^{-1/2} \cdot ?
\]

Good news: numerical experiments show that we can get very good approximation 
to $U_1(\theta_q) \approx - U_1(\theta_p)$ for $p<q$ successive primes. This 
is fantastic!

Numerical experiments suggest that we can enforce 
\[
	|U_1(\theta_p)+U_1(\theta_q)| \ll \frac{\log p}{p} .
\]
\end{proof}

Let $(X,\mu)$ be a topological measure space. Suppose $g$ is a non-trivial 
automorphism of $X$, such that $g_\ast \mu = \mu$. Suppose $g^2=1$. If we 
want to minimize 
\[
	\left| \sum_{p\leqslant x} f(x_p)\right| ,
\]
while letting the discrepancy of $\{x_p\}$ vary arbitrarily. Suppose we can 
find a ``good'' subset $U\subset X$ such that $X = U \sqcup g U$. Choose 
$x_{p_\odd}\in U$ to control the discrepancy, and then choose 
$x_{p_\even} \approx g(x_{p_\odd})$. For any $f\in C^\infty(X)$ such that 
$g^\ast f = - f$. Then 
\[
	\sum_{p\leqslant x} f(x_p) = \sum (f(x_{p_\even}) + f(x_{p_\odd})) \approx \sum 0 .
\]

We know that near $\theta=0$, 
\[
	U_n(\theta) = n + C_n \theta^2 + O(\theta^3) .
\]
(I think this will hold for any $f$ with $\int f=0$ and
$f(\pi-\theta)=f(\theta)$.)





\section{Precise method}

Let $\{p_1,p_2,\dots,\}$ be an enumeration of the rational primes. Given 
$x\in \bR$, write $\sum_{p_\odd \leqslant x} a_p$ for the sum of all $a_p$ 
for $p_i\leqslant x$ with $i$ odd, and similarly for 
$\sum_{p_\even\leqslant x}$. Suppose we have chosen 
$\theta_{p_\odd}\in [0,\pi/2)$ so that 
$\disc(\{\theta_{p_\odd}\}_{p_\odd \leqslant x})$ decays as desired. Suppose 
we choose $\theta_{p_\even} \approx \pi - \theta_{p_\odd}$. That is, for 
$p < q$ successive primes with $p=p_i$, $i$ odd, we'll choose 
$\theta_q \approx \pi - \theta_p$. 

We know that $\theta_p = \cos^{-1}\left(\frac{a_p}{2\sqrt p}\right)$ for 
some $a_p\in \bZ$ with $|a_p| \leqslant 2\sqrt p$. We want to choose 
$\theta_q\approx \pi-\theta_p$, i.e.~
\begin{align*}
	\cos^{-1}\left(\frac{a_q}{2\sqrt q}\right) 
		&\approx \pi - \cos^{-1}\left(\frac{a_p}{2\sqrt p}\right) \\
	\frac{a_q}{2\sqrt q} 
		&\approx -\frac{a_p}{2\sqrt p} .
\end{align*}
since $\cos(\pi - \cos^{-1}(x)) = - x$. We can guarantee that 
\[
	\left|\frac{a_q}{2\sqrt q} + \frac{a_p}{2\sqrt p}\right| \leqslant \frac{1}{\sqrt q} .
\]

Claim: if $x,y$ are ``further than $\epsilon$'' from $\pm 1$ and 
$|x-y|<\epsilon$, then 
$|\cos^{-1}(x) - \cos^{-1}(y)| \leqslant \sqrt \epsilon$. 
(Have checked with Wolfram Alpha, prove later.)

In conclusion, for each successive primes $p=p_\odd < q=p_\even$, if there 
is $\theta_p\in \Theta_p(l)$ chosen already, we can also choose 
$\theta_q\in \Theta_q(l)$ so that 
\[
	|\theta_q - (\pi - \theta_p)| \ll l p^{-1/4} .
\]
This is all that is needed, since we're looking at $f$ that is of the form 
\[
	f(\theta) = f(0) + C \theta^2 + O(\theta^3)
\]
for $\theta$ close to zero. (In fact, this is true for \emph{all} smooth, 
Weyl-invariant $f$, whether or not they satisfy $f(\theta) = -f(\pi-\theta)$.)
The squaring ``pushes the difference'' back to 
$p^{-1/2}$. That is, for $\theta,\varphi$ close to zero, but at least 
$\epsilon$ away from zero, we have 
\[
	|f(\theta) - f(\varphi)| \ll |\theta-\varphi|^2 .
\]
Now the question is, if $\theta_q \approx \pi - \theta_p$, how close is the 
discrepancy of $\{\theta_{p_\odd}\}$ and $\{\theta_{p_\even}\}$?

Better, how close are 
\[
	\# \{p_\odd \leqslant x : \theta_{p_\odd} \leqslant t\} \qquad \textnormal{and}\qquad \# \{p_\odd \leqslant x : \theta_{p_\odd} \leqslant t\} ?
\]
We know that $|\theta_p - \theta_q| \ll p^{-1/4}$. Actually, all we need is 
that if $\disc(\{\theta_{p_\odd}\}) \to 0$, then also 
$\disc(\{\theta_{p_\even}\}) \to 0$. 

Suppose we have two sequences $\{x_n\}$ and $\{y_n\}$ such that  
$\disc(\{x_n\}_{n\leqslant N}) \sim \frac{1}{\log N}$, and also 
$|x_n - y_n| \leqslant n^{-1/4}$. For some really big $N$, choose 
$M<N$, ideally $M\approx \log N$. 

Look at 
\[
	\limsup_{N\to \infty} \disc(\{y_n\}_{M\leqslant n \leqslant N}) \leqslant M^{-1/4} .
\]
With complete generality, we have:
\[
	\left|\disc(\{x_n\}_{n\leqslant N}) - \disc(\{x_n\}_{M\leqslant n\leqslant N})\right| \ll \frac{1}{M}
\]
This is all we need.

--- --- --- --- --- --- --- --- --- --- --- --- 

\begin{theorem}
Let $\bx$ and $\by$ be sequences in $\bR$ such that 
\begin{enumerate}
\item
$\disc(\bx^N,\nu) \to 0$. 
\item
$\|\bx_{\geqslant N} - \by_{\geqslant N}\|_\infty \to 0$. 
\end{enumerate}
Then $\disc(\by^N, \nu) \to 0$ as well. 
\end{theorem}
\begin{proof}
Recall our notation. First, $\bx^N$ is the measure 
\[
	\int f\, \dd\bx^N = \frac{1}{N} \sum_{n\leqslant N} f(x_n) ,
\]
while $\bx_{\geqslant N} = (x_N, x_{N+1}, \dots)$. The discrepancy between 
two measures $\mu$ and $\nu$ on $\bR$ is 
\[
	\disc(\mu,\nu) = \sup_{x\in \bR} |\cdf_{\mu}(x) - \cdf_\nu(x)| .
\]
Finally, the norm in line two is the supremum norm 
\[
	\| x_{\geqslant N} - y_{\geqslant N} \|_\infty = \sup_{n\geqslant N} |x_n - y_n| .
\]
Note that the discrepancy satisfies the triangle inequality, namely 
\[
	\disc(\mu,\xi) \leqslant \disc(\mu,\nu) + \disc(\nu,\xi) .
\]
Moreover, for $M < N$, write $\bx^{M,N}$ for the measure 
\[
	\int f\, \dd\bx^{M,N} = \frac{1}{N - M} \sum_{M < n \leqslant N} f(x_n) .
\]
First, note that $\disc(\bx^N,\bx^{M,N}) \leqslant \frac M N$, because the 
two finite sets in question differ by $M$ elements, so the cumulative 
distribution functions can differ by at most $\frac M N$. Now note that:
\[
	\disc(\by^N, \nu) 
		\leqslant \disc(\by^N, \by^{M,N}) + \disc(\by^{M,N}, \bx^{M,N}) + \disc(\bx^{M,N}, \bx^N) + \disc(\bx^N,\nu) .
\]
Let $M=o(N)$ but still have $M\to \infty$. Then the first and third terms 
converge to zero. The fourth term converges to zero by hypothesis, so all 
we need is to consider the second term. 

[Proof is invalid in this generality! We only have: if $\nu$ is continuous 
measure, then 
\[
	|\disc(\bx^N, \nu) - \disc(\by^N,\nu)| \ll_\nu \| \bx_{\leqslant N} - \by_{\leqslant N}\|_ \infty .
\]]
\end{proof}

More generally, 
\[
	\disc(\mu_{S\sqcup T}, \mu_S) = \|\cdf_{\mu_{S\sqcup T}} - \cdf_{\mu_S}\|_\infty \leqslant \frac{\# T}{\# S} .
\]
Moreover,  
\[
	\disc(\by^N,\nu) \leqslant \disc(\by^N, \bx^N) + \disc(\bx^N, \nu) .
\]
So to prove $\disc(\by^N,\nu)\to 0$, it suffices in fact to prove that 
$\disc(\bx^N,\by^N) \to 0$. Here, we have a chain of inequalities (where 
we write $\bx^{M,N}$ for the measure corresponding to 
$\{x_n\}_{M\leqslant N \leqslant M}$: 
\[
	\disc(\bx^N,\by^N) \leqslant \disc(\bx^N,\bx^{M,N}) + \disc(\bx^{M,N},\by^{M,N}) + \disc(\by^{M,N},\by^N) .
\]
Now, it is known that 
\[
	\disc(\bx^{M,N},\by^{M,N}) \ll M^{-1/4} ,
\]
Now, $\disc(\bx^N, \bx^{M,N}) \leqslant \frac{M}{N - M}$, so we can set 
$M\approx \sqrt N$ to get 
\[
	\disc(\bx^N, \by^N) \ll \frac{\sqrt N}{N - \sqrt N} + N^{-1/4} \to 0 .
\]

--- --- --- --- --- --- --- --- --- --- --- --- --- --- --- --- 
\[
	|\disc(\by^{M,N},\nu)  - \disc(\bx^{M,N},\nu)| \ll_\nu \|\bx_{\geqslant M} - \by_{\geqslant M}\|_\infty .
\]
Thus 
\[
	\disc(\by^{M,N},\nu) \ll_\nu \disc(\bx^{M,N},\nu) + \|\bx_{\geqslant M} - \by_{\geqslant M}\|_\infty
\]

\begin{lemma}
Let $\bx$ be a sequence in $\bR$. For $M < N$, 
\[
	\disc(\bx^N, \bx^{M,N}) \leqslant \frac{2 M}{N} .
\]
\end{lemma}
\begin{proof}
We know that 
\[
	|\#\{n\leqslant N : x_n \leqslant t\} - \#\{M < n \leqslant N : x_n \leqslant t\}| \leqslant M .
\]
Thus for all $t\in \bR$, 
\[
	|\cdf_{\bx^N}(t) - \frac{N - M}{N} \cdf_{\bx^{M,N}}(t)|\leqslant \frac M N .
\]
Now just do a simple computation, with $|\cdot| = \|\cdot\|_\infty$:
\begin{align*}
	|\cdf_{\bx^N} - \cdf_{\bx^{M,N}}| 
		&\leqslant \left|\cdf_{\bx^N} - \frac{N - M}{N} \cdf_{\bx^{M,N}}\right| + \left|\frac{N - M}{N} \cdf_{\bx^{M,N}} - \cdf_{\bx^{M,N}}\right| \\
		&\leqslant \frac M N + \frac M N .
\end{align*}
\end{proof}

\begin{lemma}
Let $\bx$ and $\by$ be sequences in $\bR$. Suppose $\nu = f\, \dd x$ for a 
continuous function $f$. Then 
\[
	|\disc(\bx^N,\nu) - \disc(\by^N,\nu)| \leqslant 2\epsilon\|f\|_\infty + \frac{\# \{n\leqslant N : |x_n - y_n| > \epsilon\}}{N} .
\]
\end{lemma}
\begin{proof}
It is actually sufficient to prove that 
\[
	\disc(\bx^N,\nu) \leqslant \disc(\by^N,\nu) + 2\epsilon \|f\|_\infty + \frac{\# \{n\leqslant N : |x_n - y_n| > \epsilon\}}{N} .
\]
\end{proof}





\end{document}
